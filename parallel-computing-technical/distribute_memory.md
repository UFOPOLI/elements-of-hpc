# Distributed Memory

Parallel computing is a type of computing in which the task is divided in several sub-tasks, which are independent of each other and can be executed simultaneously. Most of the computing problems are not trivially parallelizable, which means that the sub-tasks need to have access from time to time to some of the results computed by other sub-tasks. The way the sub-tasks exchange the needed information depends on the available hardware.

The distributed memory refers to physical memory device which is physically locate in another node. Programatically each sub-tasks has its own memory space and the exchange with other sub-tasks is done via network communications. In general each sub-tasks is assigned to a different node (we note that while for the pointof view of hardware the memory is shared, programatically it can be used distributed). The price of the hardware is relatively low. Theoretically there are no limitation to the size of the problem. If the memory requirements increases more sub-tasks are created and more nodes are used. Each sub-tasks having its own separate memory-space avoids data races and implicitly less locks are needed. The disadvantages of the parallel computing with distributed memory is that the more sub-tasks are created the larger is the time spent in the communications due to communication overheads. Also the programming become more complex, the developer is responsible for establishing the communications.
