# Distributed Memory

Parallel computing is a type of computing in which the task is divided in several sub-tasks, which are independent of each other and can be executed simultaneously. Most of the computing problems are not trivially parallelizable, which means that the sub-tasks need to have access from time to time to some of the results computed by other sub-tasks. The way the sub-tasks exchange the needed information depends on the available hardware. 

The distributed memory refers to physical memory device which is physically locate in another node. Programatically each sub-tasks has its own memory space and the exchange with other sub-tasks is done via network communications. In general each sub-taks is assigned to a different node. The price of the hardware is relatevely low. Theoretically there are no limitation to the size of the problem. If the memory requirements increases more sub-tasks are created and more nodes are used. Each sub-taks having its own separarete memory-space avoids data races and implicitly less locks are needed. The disatvantages of the parallel computing with distributed memory is that the more sub-tasks are created the larger is the time spent in the communications due to communication overheads. Also the programming become more complex, the developer is responsible for establishing the communications. 
