# Shared Memory

Parallel computing is a type of computing in which the task is divided in several sub-tasks, which are independent of each other and can be executed simultaneously. Most of the computing problems are not trivially parallelizable, which means that the sub-tasks need to have access from time to time to some of the results computed by other sub-tasks. The way the sub-tasks exchange the needed information depends on the available hardware. 

The shared memory refers to physical memory device which can be access by more than on computing core. Programatically the shared memory is seen as a space in the memory allocated to a program which can be access by a group of threads working together, without additional communication. All threads in a group can write and read from a given location in memory. When threads access a specific location, the programmer has to insure that the data needed is already written to that location there are no race conditions. Examples of shared memory are the CPU cache which is shared by all cores in the same CPU and computer main memory which is shared my all the cores on the same motherboard. (In the case of GPU shared memory is used to refer to the local memory available on each SM, the threads in a block can access and shared data with each other via the mentioned.) There are several advantages of using shared memory anf it offers good perfomance. The data exchange between sub-tasks is very fast. Developing programs is relatively simple. There are also disatavantages. The size of the problem is limited to the size of the share memory. The price and the complexity of the hardware increases with size of the shared memory. From the point of view of proggramming, the data racince and deadlock are more subtle and difficult to detect by the programmer.
