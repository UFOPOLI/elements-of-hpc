# Shared Memory

Parralel computing is a type of computing in which the task is divided in several sub-tasks, which are independent of each other and can be executed simultenously. Most of the computing problems are not trivially parallizable, which means that the sub-tasks need to have access from time to time to some of the results computed by other sub-taks. The way the sub-tasks exchange the needed information depends on the available hardware. 
The shared memory refers to physical memory device which can be access by more than on computing core. Programatically the shared memory is seen as a space in the memmory allocated to a program which can be access by a group of threads working together, **without additional communication**. All threads in a group can write and read from a given location in memory. When threads access a specific location, the programmer has to insure that the data needed is already written to that location there are no __race conditions__. 
Examples of shared memory are the CPU cache which is shared by all cores in the same CPU and computer main memory which is shared my all the cores on the same motherboard. (In the case of GPU *shared memory* is used to refer to the local memory availabl on each SM, the threads in a block can access and shared data with each other via the mentioned.)
Using shared memory can make the data exchange between sub-taks very fast, however there are physical and costs limitations to it. The larger the shared memory the more complex and more expensive is to build it.
